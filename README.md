# Deep learning reading list from Ilya Sutskever
> 深度学习精炼秘笈

> til, Ilya sutskever gave john carmack this reading list of approx 30 research papers and said, ‘If you really learn all of these, you’ll know 90% of what matters today.’
<br>

[[Twitter Post]](https://twitter.com/keshavchan/status/1787861946173186062) [[Arc.net Link]](https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE)
<br>

## 

- **The Annotated Transformer.** Sasha Rush, et al. [[Blog]](https://nlp.seas.harvard.edu/annotated-transformer/) [[GitHub]](https://github.com/harvardnlp/annotated-transformer/)
- **The First Law of Complexodynamics.** Scott Aaronson. [[Blog]](https://scottaaronson.blog/?p=762)
- **The Unreasonable Effectiveness of Recurrent Neural Networks.** Andrej Karpathy. [[Blog]](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- **Understanding LSTM Networks.** Christopher Olah. [[Blog]](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- **Recurrent Neural Network Regularization.** Wojciech Zaremba, et al. [[arXiv]](https://arxiv.org/abs/1409.2329)
- **Keeping Neural Networks Simple by Minimizing the Description Length of the Weights.** Geoffrey E. Hinton and Drew van Camp. [[pdf]](https://www.cs.toronto.edu/~hinton/absps/colt93.pdf)
- **Pointer Networks.** Oriol Vinyals, et al. [[arXiv]](https://arxiv.org/abs/1506.03134)
- **ImageNet Classification with Deep Convolutional Neural Networks.** Alex Krizhevsky, et al. [[pdf]](https://sing.stanford.edu/curis-fellowships/rh/vision-dnn.pdf)
- **Order Matters: Sequence to sequence for sets.** Oriol Vinyals, et al. [[arXiv]](https://arxiv.org/abs/1511.06391)
- **GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism.** Yanping Huang, et al. [[arXiv]](https://arxiv.org/abs/1811.06965v5)
- **Deep Residual Learning for Image Recognition.** Kaiming He, et al. [[Presentation]](https://kaiminghe.github.io/cvpr16resnet/cvpr2016_deep_residual_learning_kaiminghe.pdf) [[arXiv]](https://arxiv.org/abs/1512.03385)
- **Multi-Scale Context Aggregation by Dilated Convolutions.** Fisher Yu and Vladlen Koltun. [[arXiv]](https://arxiv.org/abs/1511.07122)
- **Neural Message Passing for Quantum Chemistry.** Justin Gilmer, et al. [[arXiv]](https://arxiv.org/abs/1704.01212)
- **Attention Is All You Need.** Ashish Vaswani, et al. [[arXiv]](https://arxiv.org/abs/1706.03762)
- **Neural Machine Translation by Jointly Learning to Align and Translate.** Dzmitry Bahdanau, et al. [[arXiv]](https://arxiv.org/abs/1409.0473)
- **Identity Mappings in Deep Residual Networks.** Kaiming He, et al. [[arXiv]](https://arxiv.org/abs/1603.05027)
- **A simple neural network module for relational reasoning.** Adam Santoro, et al. [[arXiv]](https://arxiv.org/abs/1706.01427)
- **Variational Lossy Autoencoder.** Xi Chen, et al. [[arXiv]](https://arxiv.org/abs/1611.02731)
- **Relational recurrent neural networks.** Adam Santoro, et al. [[arXiv]](https://arxiv.org/abs/1806.01822)
- **Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton.** Scott Aaronson, et al. [[arXiv]](https://arxiv.org/abs/1405.6903)
- **Neural Turing Machines.** Alex Graves, et al. [[arXiv]](https://arxiv.org/abs/1410.5401)
- **Deep Speech 2: End-to-End Speech Recognition in English and Mandarin.** Dario Amodei, et al. [[arXiv]](https://arxiv.org/abs/1512.02595)
- **Scaling Laws for Neural Language Models.** Jared Kaplan, et al. [[arXiv]](https://arxiv.org/abs/2001.08361)
- **A Tutorial Introduction to the Minimum Description Length Principle.** Peter Grunwald. [[pdf]](https://arxiv.org/pdf/math/0406077)
- **Machine Super Intelligence.** Shane Legg. [[Presentation]](https://pdfs.semanticscholar.org/e758/b579456545f8691bbadaf26bcd3b536c7172.pdf)
- **Kolmogorov Complexity and Algorithmic Randomness.** A.Shen, V. A. Uspensky, and N. Vereshchagin. [[pdf]](https://www.lirmm.fr/~ashen/kolmbook-eng-scan.pdf) Chapter 14.
- **CS231n: Convolutional Neural Networks for Visual Recognition.** [[CS231n Home]](https://cs231n.stanford.edu/) [[Course Notes]](https://cs231n.github.io/)
